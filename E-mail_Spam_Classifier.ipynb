{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95ef2feb-c2cf-4f33-a98a-71b82dde1c9d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Project Flow of ML Model Building – Email Spam Classifier\n",
    "\n",
    "## 1. Text Cleaning\n",
    "Feature extraction requires normalized text.\n",
    "- Lowercase\n",
    "- Remove URLs\n",
    "- Remove numbers\n",
    "- Remove punctuation\n",
    "- Tokenization\n",
    "\n",
    "## 2. EDA (Exploratory Data Analysis)\n",
    "Initial analysis phase to understand dataset characteristics.\n",
    "**Key Tasks:**\n",
    "- Class distribution (Spam vs Ham)\n",
    "- Email length analysis\n",
    "- Word frequency visualization\n",
    "- Detect anomalies or imbalance\n",
    "**Purpose:** Identify patterns and validate modeling assumptions.\n",
    "\n",
    "## 3. Text Pre-processing\n",
    "Transforms raw email text into machine-readable features.\n",
    "- Lowercasing text  \n",
    "- Remove punctuation & stopwords  \n",
    "- Tokenization  \n",
    "- Stemming/Lemmatization  \n",
    "- Vectorization using **TF-IDF / Bag of Words**  \n",
    "\n",
    "## 4. Model Building\n",
    "Training the classification algorithms.\n",
    "- Train–test split  \n",
    "- Feature extraction from text  \n",
    "- Train models:\n",
    "  - Naïve Bayes  \n",
    "  - Logistic Regression  \n",
    "  - SVM  \n",
    "- Fit on vectorized dataset\n",
    "\n",
    "## 5. Model Evaluation\n",
    "Assess model effectiveness.\n",
    "**Metrics:**\n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F1-Score  \n",
    "- Confusion Matrix  \n",
    "**Focus:** Balance between spam detection and false positives.\n",
    "\n",
    "## 6. Improvement\n",
    "Performance optimization stage.\n",
    "- Hyperparameter tuning  \n",
    "- N-gram features  \n",
    "- Handle class imbalance  \n",
    "- Ensemble / advanced models \n",
    "\n",
    "## 7. User Interface (Website)\n",
    "Front-end system for predictions.\n",
    "- Email text input  \n",
    "- Predict button  \n",
    "- Output: Spam / Not Spam  \n",
    "- Built with **HTML, CSS, JS + Flask/Django**  \n",
    "\n",
    "## 8. Deployment on Heroku\n",
    "Cloud hosting for public access.\n",
    "- Upload Flask app + model files  \n",
    "- Add `requirements.txt`  \n",
    "- Add `Procfile`  \n",
    "- Deploy via Git  \n",
    "- Generate live public URL  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "529e47fa-6188-4eee-9183-8423faf80a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f2fc31-9a12-4719-94f1-00dfeaa9833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('enron_spam_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65b43c9f-9033-41fe-91d4-61c535265036",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message ID</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Message</th>\n",
       "      <th>Spam/Ham</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28557</th>\n",
       "      <td>28557</td>\n",
       "      <td>wild goose storage inc . expansion open season</td>\n",
       "      <td>please watch for an important e - mail in the ...</td>\n",
       "      <td>ham</td>\n",
       "      <td>2001-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3114</th>\n",
       "      <td>3114</td>\n",
       "      <td>hpl nom for april 25 , 2001</td>\n",
       "      <td>( see attached file : hplno 425 . xls )\\n- hpl...</td>\n",
       "      <td>ham</td>\n",
       "      <td>2001-04-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10345</th>\n",
       "      <td>10345</td>\n",
       "      <td>in the heart of your business !</td>\n",
       "      <td>corporate image can say a lot of things about ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>2005-06-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Message ID                                         Subject  \\\n",
       "28557       28557  wild goose storage inc . expansion open season   \n",
       "3114         3114                     hpl nom for april 25 , 2001   \n",
       "10345       10345                 in the heart of your business !   \n",
       "\n",
       "                                                 Message Spam/Ham        Date  \n",
       "28557  please watch for an important e - mail in the ...      ham  2001-04-09  \n",
       "3114   ( see attached file : hplno 425 . xls )\\n- hpl...      ham  2001-04-24  \n",
       "10345  corporate image can say a lot of things about ...     spam  2005-06-30  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d4fe7bf-9a32-438b-8336-4a969b16f9bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33716, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47ad00df-85a6-4f59-a25f-0502f1a27672",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\suraj\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\suraj\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\suraj\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\suraj\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\suraj\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\suraj\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6080f1e-286c-4cfa-b7fd-a38566fd58c8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\suraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\suraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Cleaning\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b844f239-ebbe-49ef-86e2-03fb3a0ba7c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_email(text):\n",
    "    \n",
    "    #1 Remove email headers (Message-ID, Date, etc.)\n",
    "    text = re.sub(r'^(Message-ID|Date|From|To|Subject):.*\\n?', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    #2 Remove MIME boundaries\n",
    "    text = re.sub(r'--\\S+', '', text)\n",
    "    \n",
    "    #3 Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    \n",
    "    #4 Replace URLs with token\n",
    "    text = re.sub(r'http\\S+|www\\S+', ' <URL> ', text)\n",
    "    \n",
    "    #5 Replace email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', ' <EMAIL> ', text)\n",
    "    \n",
    "    #6️ Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #7️ Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    #8️ Remove digits (optional)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    #9️ Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    #10 Tokenization\n",
    "    tokens = text.split()\n",
    "    \n",
    "    #11 Remove stopwords + short words + lemmatize\n",
    "    cleaned_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stop_words and len(word) > 2:\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            cleaned_tokens.append(word)\n",
    "    \n",
    "    return \" \".join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0cd34ac-5ba5-4d46-96a4-07d85fd3e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['combined_text'] = df['Subject'].fillna('') + \" \" + df['Message'].fillna('')\n",
    "df['cleaned_text'] = df['combined_text'].apply(clean_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1c3f507-a2e5-446a-95ca-4430933d1949",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>combined_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>christmas tree farm pictures</td>\n",
       "      <td>christmas tree farm picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vastar resources , inc . gary , production from the high island larger block a - 1 # 2 commenced on\\nsaturday at 2 : 00 p . m . at about 6 , 500 gross . carlos expects between 9 , 500 and\\n10 , 000 gross for tomorrow . vastar owns 68 % of the gross production .\\ngeorge x 3 - 6992\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by george weissman / hou / ect on 12 / 13 / 99 10 : 16\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\ndaren j farmer\\n12 / 10 / 99 10 : 38 am\\nto : carlos j rodriguez / hou / ect @ ect\\ncc : george weissman / hou / ect @ ect , melissa graves / hou / ect @ ect\\nsubject : vastar resources , inc .\\ncarlos ,\\nplease call linda and get everything set up .\\ni ' m going to estimate 4 , 500 coming up tomorrow , with a 2 , 000 increase each\\nfollowing day based on my conversations with bill fischer at bmar .\\nd .\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by daren j farmer / hou / ect on 12 / 10 / 99 10 : 34\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\nenron north america corp .\\nfrom : george weissman 12 / 10 / 99 10 : 00 am\\nto : daren j farmer / hou / ect @ ect\\ncc : gary bryan / hou / ect @ ect , melissa graves / hou / ect @ ect\\nsubject : vastar resources , inc .\\ndarren ,\\nthe attached appears to be a nomination from vastar resources , inc . for the\\nhigh island larger block a - 1 # 2 ( previously , erroneously referred to as the\\n# 1 well ) . vastar now expects the well to commence production sometime\\ntomorrow . i told linda harris that we ' d get her a telephone number in gas\\ncontrol so she can provide notification of the turn - on tomorrow . linda ' s\\nnumbers , for the record , are 281 . 584 . 3359 voice and 713 . 312 . 1689 fax .\\nwould you please see that someone contacts linda and advises her how to\\nsubmit future nominations via e - mail , fax or voice ? thanks .\\ngeorge x 3 - 6992\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by george weissman / hou / ect on 12 / 10 / 99 09 : 44\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\n\" linda harris \" on 12 / 10 / 99 09 : 38 : 43 am\\nto : george weissman / hou / ect @ ect\\ncc :\\nsubject : hi a - 1 # 2\\neffective 12 - 11 - 99\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| mscf / d | min ftp | time |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 4 , 500 | 9 , 925 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 6 , 000 | 9 , 908 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 8 , 000 | 9 , 878 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 10 , 000 | 9 , 840 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 12 , 000 | 9 , 793 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 14 , 000 | 9 , 738 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 16 , 000 | 9 , 674 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 18 , 000 | 9 , 602 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 20 , 000 | 9 , 521 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 22 , 000 | 9 , 431 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 24 , 000 | 9 , 332 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 26 , 000 | 9 , 224 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 28 , 000 | 9 , 108 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 30 , 000 | 8 , 982 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 32 , 000 | 8 , 847 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 34 , 000 | 8 , 703 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 36 , 000 | 8 , 549 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |</td>\n",
       "      <td>vastar resource inc gary production high island larger block commenced saturday gross carlos expects gross tomorrow vastar owns gross production george forwarded george weissman hou ect daren farmer carlos rodriguez hou ect ect george weissman hou ect ect melissa graf hou ect ect subject vastar resource inc carlos please call linda get everything set going estimate coming tomorrow increase following day based conversation bill fischer bmar forwarded daren farmer hou ect enron north america corp george weissman daren farmer hou ect ect gary bryan hou ect ect melissa graf hou ect ect subject vastar resource inc darren attached appears nomination vastar resource inc high island larger block previously erroneously referred well vastar expects well commence production sometime tomorrow told linda harris get telephone number gas control provide notification turn tomorrow linda number record voice fax would please see someone contact linda advises submit future nomination via mail fax voice thanks george forwarded george weissman hou ect linda harris george weissman hou ect ect subject effective mscf min ftp time hour hour hour hour hour hour hour hour hour hour hour hour hour hour hour hour hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>calpine daily gas nomination - calpine daily gas nomination 1 . doc</td>\n",
       "      <td>calpine daily gas nomination calpine daily gas nomination doc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         combined_text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        christmas tree farm pictures    \n",
       "1  vastar resources , inc . gary , production from the high island larger block a - 1 # 2 commenced on\\nsaturday at 2 : 00 p . m . at about 6 , 500 gross . carlos expects between 9 , 500 and\\n10 , 000 gross for tomorrow . vastar owns 68 % of the gross production .\\ngeorge x 3 - 6992\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by george weissman / hou / ect on 12 / 13 / 99 10 : 16\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\ndaren j farmer\\n12 / 10 / 99 10 : 38 am\\nto : carlos j rodriguez / hou / ect @ ect\\ncc : george weissman / hou / ect @ ect , melissa graves / hou / ect @ ect\\nsubject : vastar resources , inc .\\ncarlos ,\\nplease call linda and get everything set up .\\ni ' m going to estimate 4 , 500 coming up tomorrow , with a 2 , 000 increase each\\nfollowing day based on my conversations with bill fischer at bmar .\\nd .\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by daren j farmer / hou / ect on 12 / 10 / 99 10 : 34\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\nenron north america corp .\\nfrom : george weissman 12 / 10 / 99 10 : 00 am\\nto : daren j farmer / hou / ect @ ect\\ncc : gary bryan / hou / ect @ ect , melissa graves / hou / ect @ ect\\nsubject : vastar resources , inc .\\ndarren ,\\nthe attached appears to be a nomination from vastar resources , inc . for the\\nhigh island larger block a - 1 # 2 ( previously , erroneously referred to as the\\n# 1 well ) . vastar now expects the well to commence production sometime\\ntomorrow . i told linda harris that we ' d get her a telephone number in gas\\ncontrol so she can provide notification of the turn - on tomorrow . linda ' s\\nnumbers , for the record , are 281 . 584 . 3359 voice and 713 . 312 . 1689 fax .\\nwould you please see that someone contacts linda and advises her how to\\nsubmit future nominations via e - mail , fax or voice ? thanks .\\ngeorge x 3 - 6992\\n- - - - - - - - - - - - - - - - - - - - - - forwarded by george weissman / hou / ect on 12 / 10 / 99 09 : 44\\nam - - - - - - - - - - - - - - - - - - - - - - - - - - -\\n\" linda harris \" on 12 / 10 / 99 09 : 38 : 43 am\\nto : george weissman / hou / ect @ ect\\ncc :\\nsubject : hi a - 1 # 2\\neffective 12 - 11 - 99\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| mscf / d | min ftp | time |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 4 , 500 | 9 , 925 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 6 , 000 | 9 , 908 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 8 , 000 | 9 , 878 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 10 , 000 | 9 , 840 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 12 , 000 | 9 , 793 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 14 , 000 | 9 , 738 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 16 , 000 | 9 , 674 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 18 , 000 | 9 , 602 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 20 , 000 | 9 , 521 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 22 , 000 | 9 , 431 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 24 , 000 | 9 , 332 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 26 , 000 | 9 , 224 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 28 , 000 | 9 , 108 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 30 , 000 | 8 , 982 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 32 , 000 | 8 , 847 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 34 , 000 | 8 , 703 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |\\n| | | |\\n| 36 , 000 | 8 , 549 | 24 hours |\\n| | | |\\n| - - - - - - - - + - - - - - - - - - - + - - - - - - - - - - - |   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  calpine daily gas nomination - calpine daily gas nomination 1 . doc   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               cleaned_text  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               christmas tree farm picture  \n",
       "1  vastar resource inc gary production high island larger block commenced saturday gross carlos expects gross tomorrow vastar owns gross production george forwarded george weissman hou ect daren farmer carlos rodriguez hou ect ect george weissman hou ect ect melissa graf hou ect ect subject vastar resource inc carlos please call linda get everything set going estimate coming tomorrow increase following day based conversation bill fischer bmar forwarded daren farmer hou ect enron north america corp george weissman daren farmer hou ect ect gary bryan hou ect ect melissa graf hou ect ect subject vastar resource inc darren attached appears nomination vastar resource inc high island larger block previously erroneously referred well vastar expects well commence production sometime tomorrow told linda harris get telephone number gas control provide notification turn tomorrow linda number record voice fax would please see someone contact linda advises submit future nomination via mail fax voice thanks george forwarded george weissman hou ect linda harris george weissman hou ect ect subject effective mscf min ftp time hour hour hour hour hour hour hour hour hour hour hour hour hour hour hour hour hour  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             calpine daily gas nomination calpine daily gas nomination doc  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df[['combined_text', 'cleaned_text']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bd51dad-5d45-4a50-b58c-6129ddbd8d64",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         ham\n",
       "1         ham\n",
       "2         ham\n",
       "3         ham\n",
       "4         ham\n",
       "         ... \n",
       "33711    spam\n",
       "33712    spam\n",
       "33713    spam\n",
       "33714    spam\n",
       "33715    spam\n",
       "Name: Spam/Ham, Length: 33716, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1 Train-Test Split (80-20 ratio)\n",
    "\n",
    "df['cleaned_text']\n",
    "df['Spam/Ham']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5337976f-0b01-42bd-98dd-8667920c7f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['cleaned_text']\n",
    "y = df['Spam/Ham']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41def12b-1868-4aec-95bb-d13a275bf91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 TF-IDF Vectorization (Fit Only on Train)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=9000)\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b991b23e-4ee9-41cc-8f90-687caf7004c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aaa', 'aaron', 'abacha', 'abacus', 'abandon', 'abandoned', 'abb',\n",
       "       'abbott', 'abc', 'abdominal'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "175b9d49-9ea4-42b0-8579-2207167959e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.9876927639383155\n"
     ]
    }
   ],
   "source": [
    "# Model 1 - Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "nb_pred = nb_model.predict(X_test_tfidf)\n",
    "nb_accuracy = accuracy_score(y_test, nb_pred)\n",
    "\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0cba2c5-f9cd-45ae-9198-e6f2d8e1db0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9887307236061684\n"
     ]
    }
   ],
   "source": [
    "# Model 2 - Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "lr_pred = lr_model.predict(X_test_tfidf)\n",
    "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\", lr_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b56136a-fcc6-4019-ac56-9162aa4383ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM Accuracy: 0.9915480427046264\n"
     ]
    }
   ],
   "source": [
    "# Model 3 - Linear Support Vector Machine (SVM)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_model = LinearSVC()\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "svm_pred = svm_model.predict(X_test_tfidf)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "\n",
    "print(\"Linear SVM Accuracy:\", svm_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37d80631-0ced-4d50-98a6-910a10189d14",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.98      0.99      3309\n",
      "        spam       0.98      0.99      0.99      3435\n",
      "\n",
      "    accuracy                           0.99      6744\n",
      "   macro avg       0.99      0.99      0.99      6744\n",
      "weighted avg       0.99      0.99      0.99      6744\n",
      "\n",
      "Naive Bayes Confusion Matrix:\n",
      "\n",
      "[[3254   55]\n",
      " [  28 3407]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize model\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Train\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict\n",
    "nb_pred = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Naive Bayes Classification Report:\\n\")\n",
    "print(classification_report(y_test, nb_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Naive Bayes Confusion Matrix:\\n\")\n",
    "print(confusion_matrix(y_test, nb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa65587e-e09d-44cc-a2eb-942386b85c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.98      0.99      3309\n",
      "        spam       0.98      1.00      0.99      3435\n",
      "\n",
      "    accuracy                           0.99      6744\n",
      "   macro avg       0.99      0.99      0.99      6744\n",
      "weighted avg       0.99      0.99      0.99      6744\n",
      "\n",
      "Logistic Regression Confusion Matrix:\n",
      "\n",
      "[[3249   60]\n",
      " [  16 3419]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize model\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict\n",
    "lr_pred = lr_model.predict(X_test_tfidf)\n",
    "\n",
    "# Classification Report\n",
    "print(\"Logistic Regression Classification Report:\\n\")\n",
    "print(classification_report(y_test, lr_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"Logistic Regression Confusion Matrix:\\n\")\n",
    "print(confusion_matrix(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8752927e-7f35-43c6-8524-cf386795d58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99      3309\n",
      "        spam       0.99      1.00      0.99      3435\n",
      "\n",
      "    accuracy                           0.99      6744\n",
      "   macro avg       0.99      0.99      0.99      6744\n",
      "weighted avg       0.99      0.99      0.99      6744\n",
      "\n",
      "SVM Confusion Matrix:\n",
      "\n",
      "[[3269   40]\n",
      " [  17 3418]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Classification Report\n",
    "print(\"SVM Classification Report:\\n\")\n",
    "print(classification_report(y_test, svm_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"SVM Confusion Matrix:\\n\")\n",
    "print(confusion_matrix(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9077cdbb-eda0-44e5-8f5f-f7ad6f292c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_email_ensemble(text, weak_threshold=0.6, strong_threshold=0.75):\n",
    "    \n",
    "    # Clean\n",
    "    cleaned = clean_email(text)\n",
    "    vectorized = vectorizer.transform([cleaned])\n",
    "    \n",
    "    # Predictions\n",
    "    nb_pred = str(nb_model.predict(vectorized)[0])\n",
    "    lr_pred = str(lr_model.predict(vectorized)[0])\n",
    "    svm_pred = str(svm_model.predict(vectorized)[0])\n",
    "    \n",
    "    # Logistic Regression probability\n",
    "    lr_prob = lr_model.predict_proba(vectorized)[0]\n",
    "    spam_index = list(lr_model.classes_).index('spam')\n",
    "    spam_confidence = lr_prob[spam_index]\n",
    "    \n",
    "    # -------- Decision Logic -------- #\n",
    "    \n",
    "    # Case 1: NB says HAM and LR is weak spam → HAM\n",
    "    if nb_pred == 'ham' and spam_confidence < weak_threshold:\n",
    "        final_pred = 'ham'\n",
    "    \n",
    "    # Case 2: LR & SVM strongly agree spam with high confidence → SPAM\n",
    "    elif lr_pred == 'spam' and svm_pred == 'spam' and spam_confidence >= strong_threshold:\n",
    "        final_pred = 'spam'\n",
    "    \n",
    "    # Otherwise safer choice\n",
    "    else:\n",
    "        final_pred = 'ham'\n",
    "    \n",
    "    return {\n",
    "        \"Naive Bayes\": nb_pred,\n",
    "        \"Logistic Regression\": lr_pred,\n",
    "        \"Linear SVM\": svm_pred,\n",
    "        \"Spam Confidence (LR)\": round(spam_confidence, 4),\n",
    "        \"Final Decision\": final_pred.upper()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d12934f5-9793-4905-afbf-1f04e2a7508a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Naive Bayes': 'spam', 'Logistic Regression': 'spam', 'Linear SVM': 'spam', 'Spam Confidence (LR)': np.float64(0.9837), 'Final Decision': 'SPAM'}\n"
     ]
    }
   ],
   "source": [
    "# Random Tests :-\n",
    "\n",
    "email_text = \"\"\"\n",
    "URGENT!!! You have won $10,000.\n",
    "Click the link below to claim now.\n",
    "\"\"\"\n",
    "\n",
    "result = predict_email_ensemble(email_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80809465-304c-412e-b07f-2a1c223267e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Naive Bayes': 'spam', 'Logistic Regression': 'spam', 'Linear SVM': 'spam', 'Spam Confidence (LR)': np.float64(0.9026), 'Final Decision': 'SPAM'}\n"
     ]
    }
   ],
   "source": [
    "email_text = \"\"\"\n",
    "Congratulations!!! You have won $5000.\n",
    "Click here to claim your prize now.\n",
    "\"\"\"\n",
    "\n",
    "result = predict_email_ensemble(email_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a240b03-f9c1-4dad-b0a1-38bd0682d5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Naive Bayes': 'ham', 'Logistic Regression': 'spam', 'Linear SVM': 'spam', 'Spam Confidence (LR)': np.float64(0.7161), 'Final Decision': 'HAM'}\n"
     ]
    }
   ],
   "source": [
    "email_text = \"\"\"\n",
    "The Team is impressed by your marketing strategy. Let's catch up for disussing a marketing campaign !\n",
    "- Marketing Head\n",
    "\"\"\"\n",
    "\n",
    "result = predict_email_ensemble(email_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "001bf120-c24a-4bc1-adc2-ab1ccc427ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Naive Bayes': 'spam', 'Logistic Regression': 'ham', 'Linear SVM': 'ham', 'Spam Confidence (LR)': np.float64(0.4697), 'Final Decision': 'HAM'}\n"
     ]
    }
   ],
   "source": [
    "email_text = \"\"\"\n",
    "It's nice connecting with you. Your refferal really helped a lot. Thanks for your support. \n",
    "\"\"\"\n",
    "\n",
    "result = predict_email_ensemble(email_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d45d9bcf-a6f0-4a1f-9063-3338e20fab08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Naive Bayes': 'spam', 'Logistic Regression': 'spam', 'Linear SVM': 'spam', 'Spam Confidence (LR)': np.float64(0.6224), 'Final Decision': 'HAM'}\n"
     ]
    }
   ],
   "source": [
    "email_text = \"\"\"\n",
    "Hey Builders,\n",
    "It feels like every week we open the laptop and the surface has expanded again.\n",
    "Memory that sticks. Agents that plan. Models that see, reason, call tools, and finish the job. What used to require heavy orchestration is becoming built-in. The shift from assistive to autonomous systems isn’t gradual—it’s compounding.\n",
    "So this moment isn’t about access. It’s about execution. If you’ve been waiting for the stack to mature, it has. And just like we say every other week: Now it’s about what you ship with it.\n",
    "\"\"\"\n",
    "result = predict_email_ensemble(email_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f1c6a0-ea63-464b-9a7d-8844cfa7c81f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
